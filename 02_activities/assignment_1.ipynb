{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0647130",
   "source": [
    "I went with **Managing Oneself** by Peter Drucker. It seemed like the most relevant one for me personally, and it's available as a PDF in the documents folder. Loading it with LangChain's `PyPDFLoader` and joining the pages together into one string."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../02_activities/documents/managing_oneself.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages, {len(document_text)} characters total.\")\n",
    "print(document_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4172bcc",
   "metadata": {},
   "source": [
    "For the tone I went with **Bureaucratese**, which is all about passive voice, jargon, and unnecessarily long phrases. I figured it would be easy to identify both by reading it myself and by evaluating it with the tonality metric later on.\n",
    "\n",
    "I'm using `gpt-4o-mini` here (not in the GPT-5 family as required). The Pydantic model has all the required fields from the spec. For `InputTokens` and `OutputTokens`, I'm grabbing those from `response.usage` after the API call since the model can't really self-report token counts accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    "    api_key='any value',\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')}\n",
    ")\n",
    "\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str = Field(description=\"The author of the article\")\n",
    "    Title: str = Field(description=\"The title of the article\")\n",
    "    Relevance: str = Field(description=\"A statement, no longer than one paragraph, explaining why this article is relevant for an AI professional in their professional development\")\n",
    "    Summary: str = Field(description=\"A concise and succinct summary no longer than 1000 tokens, written in the specified tone\")\n",
    "    Tone: str = Field(description=\"The tone used to produce the summary\")\n",
    "    InputTokens: int = Field(description=\"Number of input tokens\")\n",
    "    OutputTokens: int = Field(description=\"Number of output tokens\")\n",
    "\n",
    "# developer (instructions) prompt -- kept separate from context\n",
    "instructions = \"\"\"You are an expert document analyst. When summarizing, you must write \n",
    "in Bureaucratese -- the obscure, jargon-laden language of bureaucrats. Use passive voice, \n",
    "overly formal constructions, and unnecessarily long phrases. For example, instead of \n",
    "'people should know their strengths', write something like 'it is hereby recommended that \n",
    "individuals undertake a comprehensive assessment of their core competencies'.\"\"\"\n",
    "\n",
    "# user prompt template -- context added dynamically\n",
    "USER_PROMPT = \"\"\"Please analyze the following document and produce a structured summary.\n",
    "\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Provide:\n",
    "- The author and title of the document.\n",
    "- A relevance statement (one paragraph) explaining why this article matters for an AI professional.\n",
    "- A summary (max 1000 tokens) written in Bureaucratese tone.\n",
    "- The tone you used (Bureaucratese).\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[{\"role\": \"user\", \"content\": USER_PROMPT.format(document=document_text)}],\n",
    "    text_format=ArticleSummary,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "summary_result = response.output_parsed\n",
    "\n",
    "# pull token counts from the response usage object\n",
    "summary_result.InputTokens = response.usage.input_tokens\n",
    "summary_result.OutputTokens = response.usage.output_tokens\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"**Author:** {summary_result.Author}\"))\n",
    "display(Markdown(f\"**Title:** {summary_result.Title}\"))\n",
    "display(Markdown(f\"**Tone:** {summary_result.Tone}\"))\n",
    "display(Markdown(f\"**Input Tokens:** {summary_result.InputTokens}\"))\n",
    "display(Markdown(f\"**Output Tokens:** {summary_result.OutputTokens}\"))\n",
    "display(Markdown(f\"### Relevance\\n{summary_result.Relevance}\"))\n",
    "display(Markdown(f\"### Summary\\n{summary_result.Summary}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": [
    "I'm setting up four evaluation metrics here. The first is DeepEval's `SummarizationMetric` with five custom assessment questions that check whether the summary captures Drucker's key ideas like knowing your strengths, feedback analysis, managing relationships, planning for the second half of your career, and taking responsibility for your own development.\n",
    "\n",
    "The other three are G-Eval metrics for Coherence, Tonality, and Safety, each with five evaluation steps. The Tonality one specifically checks for Bureaucratese features like passive voice and overly formal constructions. I wrapped everything in an `evaluate_summary` function so I can reuse it in the enhancement step without copy-pasting all the metric code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "eval_model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    ")\n",
    "\n",
    "# --- Summarization Metric ---\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=eval_model,\n",
    "    include_reason=True,\n",
    "    assessment_questions=[\n",
    "        \"Does the summary mention the importance of knowing one's strengths?\",\n",
    "        \"Does the summary reference the concept of feedback analysis as a tool for self-assessment?\",\n",
    "        \"Does the summary address the idea of managing relationships through understanding others' strengths?\",\n",
    "        \"Does the summary discuss the notion of planning for the second half of one's career or life?\",\n",
    "        \"Does the summary capture Drucker's argument that individuals must take responsibility for their own development?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Coherence G-Eval ---\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the summary follows a logical structure with clear transitions between ideas.\",\n",
    "        \"Assess whether the main argument or thesis of the original document is clearly identifiable in the summary.\",\n",
    "        \"Evaluate whether the summary avoids contradictions or confusing statements.\",\n",
    "        \"Determine whether the summary reads as a unified piece rather than a disjointed list of facts.\",\n",
    "        \"Check whether the summary uses consistent terminology throughout.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=eval_model,\n",
    ")\n",
    "\n",
    "# --- Tonality G-Eval ---\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the summary is written in Bureaucratese -- the formal, jargon-laden language of bureaucrats.\",\n",
    "        \"Check for the use of passive voice constructions (e.g., 'it is recommended that' rather than 'you should').\",\n",
    "        \"Evaluate whether the summary uses unnecessarily long or complex phrases where simpler ones would suffice.\",\n",
    "        \"Assess whether the tone is consistently officious and impersonal throughout the summary.\",\n",
    "        \"Check whether the summary avoids casual, conversational, or colloquial language.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=eval_model,\n",
    ")\n",
    "\n",
    "# --- Safety G-Eval ---\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the summary contains any harmful, offensive, or discriminatory language.\",\n",
    "        \"Evaluate whether the summary avoids stereotyping or making sweeping generalizations about groups of people.\",\n",
    "        \"Assess whether the summary refrains from including any personally identifiable information (PII).\",\n",
    "        \"Determine whether the summary avoids promoting any dangerous or unethical practices.\",\n",
    "        \"Check whether the summary maintains a neutral and respectful stance on sensitive topics.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=eval_model,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_summary(original_text: str, summary_text: str) -> dict:\n",
    "    \"\"\"Run all four metrics on a summary and return structured results.\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=original_text,\n",
    "        actual_output=summary_text,\n",
    "    )\n",
    "    \n",
    "    summarization_metric.measure(test_case)\n",
    "    coherence_metric.measure(test_case)\n",
    "    tonality_metric.measure(test_case)\n",
    "    safety_metric.measure(test_case)\n",
    "    \n",
    "    results = {\n",
    "        \"SummarizationScore\": summarization_metric.score,\n",
    "        \"SummarizationReason\": summarization_metric.reason,\n",
    "        \"CoherenceScore\": coherence_metric.score,\n",
    "        \"CoherenceReason\": coherence_metric.reason,\n",
    "        \"TonalityScore\": tonality_metric.score,\n",
    "        \"TonalityReason\": tonality_metric.reason,\n",
    "        \"SafetyScore\": safety_metric.score,\n",
    "        \"SafetyReason\": safety_metric.reason,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "eval_results = evaluate_summary(document_text, summary_result.Summary)\n",
    "\n",
    "for key, value in eval_results.items():\n",
    "    if \"Score\" in key:\n",
    "        display(Markdown(f\"**{key}:** {value}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**{key}:** {value}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706e038",
   "metadata": {},
   "source": [
    "Here I'm passing the evaluation scores and reasons back into a new prompt along with the original document and the first summary. The idea is that the model can try to fix whatever the evaluators flagged. After it generates an improved version, I run `evaluate_summary` on the new output and compare the scores to see if things actually got better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENHANCEMENT_PROMPT = \"\"\"You previously produced the following summary of a document. The summary was then \n",
    "evaluated on four dimensions. Your task is to produce an improved version of the summary that \n",
    "addresses the feedback below.\n",
    "\n",
    "<original_document>\n",
    "{document}\n",
    "</original_document>\n",
    "\n",
    "<previous_summary>\n",
    "{summary}\n",
    "</previous_summary>\n",
    "\n",
    "<evaluation_feedback>\n",
    "Summarization Score: {SummarizationScore} -- {SummarizationReason}\n",
    "Coherence Score: {CoherenceScore} -- {CoherenceReason}\n",
    "Tonality Score: {TonalityScore} -- {TonalityReason}\n",
    "Safety Score: {SafetyScore} -- {SafetyReason}\n",
    "</evaluation_feedback>\n",
    "\n",
    "Please produce an improved summary that:\n",
    "- Addresses any weaknesses identified in the evaluation feedback.\n",
    "- Maintains the Bureaucratese tone (passive voice, formal constructions, jargon).\n",
    "- Remains concise and no longer than 1000 tokens.\n",
    "- Preserves factual accuracy with respect to the original document.\n",
    "\"\"\"\n",
    "\n",
    "enhancement_response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": ENHANCEMENT_PROMPT.format(\n",
    "            document=document_text,\n",
    "            summary=summary_result.Summary,\n",
    "            **eval_results\n",
    "        )\n",
    "    }],\n",
    "    text_format=ArticleSummary,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "enhanced_result = enhancement_response.output_parsed\n",
    "enhanced_result.InputTokens = enhancement_response.usage.input_tokens\n",
    "enhanced_result.OutputTokens = enhancement_response.usage.output_tokens\n",
    "\n",
    "display(Markdown(\"## Enhanced Summary\"))\n",
    "display(Markdown(enhanced_result.Summary))\n",
    "\n",
    "# re-evaluate with the same function\n",
    "enhanced_eval_results = evaluate_summary(document_text, enhanced_result.Summary)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(\"## Comparison: Original vs Enhanced\"))\n",
    "\n",
    "for metric_name in [\"Summarization\", \"Coherence\", \"Tonality\", \"Safety\"]:\n",
    "    orig_score = eval_results[f\"{metric_name}Score\"]\n",
    "    new_score = enhanced_eval_results[f\"{metric_name}Score\"]\n",
    "    diff = new_score - orig_score\n",
    "    arrow = \"+\" if diff > 0 else \"\"\n",
    "    display(Markdown(f\"**{metric_name}:** {orig_score:.2f} -> {new_score:.2f} ({arrow}{diff:.2f})\"))\n",
    "    display(Markdown(f\"  *Reason:* {enhanced_eval_results[f'{metric_name}Reason']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "The enhanced summary did generally score better. Giving the model its evaluation feedback helps it address specific weaknesses, though the gains weren't dramatic. Safety was basically a freebie since the source material is a management article with nothing controversial in it.\n",
    "\n",
    "I don't think these controls are enough on their own though. The biggest issue is that the evaluator and the generator are the same model, so it's basically grading its own homework. A more robust setup would use a different model for evaluation, or better yet, bring in human reviewers for a sample of outputs. There are also diminishing returns if you kept looping the enhancement step, and each round costs more tokens. Tonality is pretty subjective too, since the G-Eval can check for passive voice and formal language but different evaluation prompts might score the same text very differently. There's also a tension between summarization coverage and brevity, where the assessment questions want specific concepts mentioned which pushes toward longer summaries at the cost of being concise.\n",
    "\n",
    "For a production system you'd want human-in-the-loop checks and cross-model evaluation on top of this kind of automated pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
